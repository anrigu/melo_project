{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /var/folders/fh/fwc37qhn04d8sxp65hwv1kxm0000gn/T/matplotlib-lagmrzdk because the default path (/Users/gabesmithline/.matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to path: /Users/gabesmithline/Desktop/SRG/melo_project\n",
      "Added parent directory to path: /Users/gabesmithline/Desktop/SRG/melo_project/marketsim\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the marketsim directory to the path\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "# Add the simulator's directory to the path so its relative imports work\n",
    "simulator_dir = os.path.join(root_dir, \"marketsim\", \"simulator\")\n",
    "parent_dir = os.path.dirname(simulator_dir)  # This is the marketsim directory\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "print(f\"Added to path: {root_dir}\")\n",
    "print(f\"Added parent directory to path: {parent_dir}\")\n",
    "\n",
    "\n",
    "\n",
    "from marketsim.simulator.melo_simulator import MELOSimulatorSampledArrival\n",
    "from marketsim.egta.game import AbstractGame\n",
    "from marketsim.egta.symmetric_game import SymmetricGame\n",
    "from marketsim.egta.reductions.dpr import DPRGAME\n",
    "from marketsim.egta.process_data import create_symmetric_game_from_data\n",
    "from marketsim.egta.utils.eq_computation import find_equilibria\n",
    "from marketsim.egta.utils.log_multimodal import logmultinomial\n",
    "from marketsim.egta.utils.random_functions import *\n",
    "from marketsim.egta.utils.simplex_operations import *\n",
    "from marketsim.egta.schedulers.melo_scheduler import MeloScheduler\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "def generate_profiles(strategies: List[Dict[str, Any]], num_players: int) -> List[List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Generate all possible strategy profiles for a symmetric game.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    strategies : List[Dict[str, Any]]\n",
    "        List of strategy dictionaries, each containing at least 'name' and 'param'\n",
    "    num_players : int\n",
    "        Number of players in the game\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[List[Dict[str, Any]]]\n",
    "        List of strategy profiles, where each profile is a list of strategy dicts (one per player)\n",
    "    \"\"\"\n",
    "    # Generate all possible combinations of strategies with replacement\n",
    "    # This gives us counts of each strategy (e.g., 3 of strategy 0, 2 of strategy 1, etc.)\n",
    "    strategy_combinations = itertools.combinations_with_replacement(range(len(strategies)), num_players)\n",
    "    \n",
    "    profiles = []\n",
    "    \n",
    "    for combination in strategy_combinations:\n",
    "        # Convert the combination (which has indices) to a list of actual strategy objects\n",
    "        profile = [strategies[idx] for idx in combination]\n",
    "        profiles.append(profile)\n",
    "    \n",
    "    return profiles\n",
    "\n",
    "\n",
    "def generate_profiles_counts(strategies: List[Dict[str, Any]], num_players: int) -> List[Tuple[List[Dict[str, Any]], List[int]]]:\n",
    "    \"\"\"\n",
    "    Generate all possible strategy profiles with counts for a symmetric game.\n",
    "    \n",
    "    This version returns both the unique strategies and their counts in each profile,\n",
    "    which can be more efficient for some EGTA algorithms.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    strategies : List[Dict[str, Any]]\n",
    "        List of strategy dictionaries, each containing at least 'name' and 'param'\n",
    "    num_players : int\n",
    "        Number of players in the game\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[Tuple[List[Dict[str, Any]], List[int]]]\n",
    "        List of (unique_strategies, counts) tuples for each profile\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    \n",
    "    # Generate all possible combinations of strategies with replacement\n",
    "    strategy_combinations = itertools.combinations_with_replacement(range(len(strategies)), num_players)\n",
    "    \n",
    "    profiles_with_counts = []\n",
    "    \n",
    "    for combination in strategy_combinations:\n",
    "        # Count occurrences of each strategy\n",
    "        counts = Counter(combination)\n",
    "        \n",
    "        # Create lists of unique strategies and their counts\n",
    "        unique_strategies = [strategies[idx] for idx in counts.keys()]\n",
    "        strategy_counts = list(counts.values())\n",
    "        \n",
    "        profiles_with_counts.append((unique_strategies, strategy_counts))\n",
    "    \n",
    "    return profiles_with_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def run_initial_simulations(strategies: List[Dict[str, Any]], num_players: int, \n",
    "                          num_runs: int = 5, sim_time: int = 1000) -> List[List[Tuple]]:\n",
    "    \"\"\"\n",
    "    Run initial simulations for all possible strategy profiles.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    strategies : List[Dict[str, Any]]\n",
    "        List of strategies to explore\n",
    "    num_players : int\n",
    "        Number of players per simulation\n",
    "    num_runs : int\n",
    "        Number of simulation runs per profile (for statistical robustness)\n",
    "    sim_time : int\n",
    "        Simulation time steps\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[List[Tuple]]\n",
    "        List of raw data from simulation runs\n",
    "    \"\"\"\n",
    "    # Generate all possible profiles\n",
    "    profiles = generate_profiles(strategies, num_players)\n",
    "    \n",
    "    # Store simulation results\n",
    "    all_results = []\n",
    "    \n",
    "    print(f\"Running {len(profiles)} profiles with {num_runs} repetitions each...\")\n",
    "    \n",
    "    # For each profile\n",
    "    for profile_idx, profile in enumerate(profiles):\n",
    "        print(f\"Simulating profile {profile_idx+1}/{len(profiles)}: \" + \n",
    "              \", \".join([f\"{s['name']}\" for s in profile]))\n",
    "        \n",
    "        # Run multiple times for statistical robustness\n",
    "        for run in range(num_runs):\n",
    "            # Configure simulator\n",
    "            simulator = MELOSimulatorSampledArrival(\n",
    "                num_background_agents=num_players,\n",
    "                sim_time=sim_time,\n",
    "                num_assets=1,\n",
    "                lam=0.1,  # Default lambda\n",
    "                mean=100,\n",
    "                r=0.05,\n",
    "                shock_var=10,\n",
    "                q_max=10,\n",
    "                pv_var=5e6,\n",
    "                eta=0.2\n",
    "            )\n",
    "            \n",
    "            # Set up agents with their strategies\n",
    "            for agent_id, strategy in enumerate(profile):\n",
    "                # Create agent with the appropriate strategy\n",
    "                # Note: MELOSimulatorSampledArrival doesn't have a set_agent_strategy method\n",
    "                # Instead, we need to configure agents correctly during initialization\n",
    "                \n",
    "                # Since the agents are created in the simulator's __init__, we need to\n",
    "                # modify the agent's lambda parameter after creation\n",
    "                # Access the agent and configure its lambda parameter\n",
    "                if agent_id < len(simulator.agents):\n",
    "                    agent = simulator.agents[agent_id]\n",
    "                    if hasattr(agent, 'lam'):\n",
    "                        agent.lam = strategy['param']\n",
    "                    else:\n",
    "                        print(f\"Warning: Agent {agent_id} doesn't have a 'lam' attribute\")\n",
    "            \n",
    "            # Run simulation\n",
    "            simulator.run()\n",
    "            \n",
    "            # Collect payoffs\n",
    "            # This assumes the simulator returns agent profits at the end\n",
    "            # We may need to modify this based on how payoffs are accessed\n",
    "            payoffs = []\n",
    "            for agent_id in range(num_players):\n",
    "                if agent_id in simulator.agents:\n",
    "                    # Get agent's profit - this will depend on how your simulator stores this\n",
    "                    # You might need to access agent.meloProfit or similar\n",
    "                    if hasattr(simulator.agents[agent_id], 'meloProfit'):\n",
    "                        payoff = simulator.agents[agent_id].meloProfit\n",
    "                    else:\n",
    "                        # Fallback - use some other profit measure if available\n",
    "                        position = simulator.agents[agent_id].position\n",
    "                        cash = simulator.agents[agent_id].cash\n",
    "                        # Estimate payoff based on final position and cash\n",
    "                        fundamental_val = simulator.market.get_final_fundamental()\n",
    "                        payoff = position * fundamental_val + cash\n",
    "                    \n",
    "                    payoffs.append(payoff)\n",
    "                else:\n",
    "                    payoffs.append(0.0)  # Default if agent not found\n",
    "            \n",
    "            # Store profile and payoffs\n",
    "            profile_result = []\n",
    "            for agent_id, (strategy, payoff) in enumerate(zip(profile, payoffs)):\n",
    "                profile_result.append((agent_id, strategy[\"name\"], payoff))\n",
    "            \n",
    "            all_results.append(profile_result)\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 4 profiles with 3 repetitions each...\n",
      "Simulating profile 1/4: MELO_0.1, MELO_0.1, MELO_0.1\n",
      "Warning: Agent 0 doesn't have a 'lam' attribute\n",
      "Warning: Agent 1 doesn't have a 'lam' attribute\n",
      "Warning: Agent 2 doesn't have a 'lam' attribute\n",
      "At the end of the simulation we get {-12: tensor(0.), -11: tensor(0.), -10: tensor(0.), -9: tensor(0.), -8: tensor(0.), -7: tensor(0.), -6: tensor(0.), -5: tensor(0.), -4: tensor(0.), -3: tensor(0.), -2: tensor(0.), -1: tensor(0.), 0: tensor(0.), 1: tensor(0.), 2: tensor(0.), 3: tensor(0.)}\n",
      "MELO_ At the end of the simulation we get {-12: 0, -11: 0, -10: 0, -9: 0, -8: 0, -7: 0, -6: 0, -5: 0, -4: 0, -3: 0, -2: 0, -1: 0, 0: 0, 1: 0, 2: 0, 3: 0}\n",
      "Warning: Agent 0 doesn't have a 'lam' attribute\n",
      "Warning: Agent 1 doesn't have a 'lam' attribute\n",
      "Warning: Agent 2 doesn't have a 'lam' attribute\n",
      "At the end of the simulation we get {-12: tensor(0.), -11: tensor(0.), -10: tensor(0.), -9: tensor(0.), -8: tensor(0.), -7: tensor(0.), -6: tensor(0.), -5: tensor(0.), -4: tensor(0.), -3: tensor(0.), -2: tensor(0.), -1: tensor(0.), 0: tensor(0.), 1: tensor(0.), 2: tensor(0.), 3: tensor(0.)}\n",
      "MELO_ At the end of the simulation we get {-12: 0, -11: 0, -10: 0, -9: 0, -8: 0, -7: 0, -6: 0, -5: 0, -4: 0, -3: 0, -2: 0, -1: 0, 0: 0, 1: 0, 2: 0, 3: 0}\n",
      "Warning: Agent 0 doesn't have a 'lam' attribute\n",
      "Warning: Agent 1 doesn't have a 'lam' attribute\n",
      "Warning: Agent 2 doesn't have a 'lam' attribute\n",
      "At the end of the simulation we get {-12: tensor(0.), -11: tensor(0.), -10: tensor(0.), -9: tensor(0.), -8: tensor(0.), -7: tensor(0.), -6: tensor(0.), -5: tensor(0.), -4: tensor(0.), -3: tensor(0.), -2: tensor(0.), -1: tensor(0.), 0: tensor(0.), 1: tensor(0.), 2: tensor(0.), 3: tensor(0.)}\n",
      "MELO_ At the end of the simulation we get {-12: 0, -11: 0, -10: 0, -9: 0, -8: 0, -7: 0, -6: 0, -5: 0, -4: 0, -3: 0, -2: 0, -1: 0, 0: 0, 1: 0, 2: 0, 3: 0}\n",
      "Simulating profile 2/4: MELO_0.1, MELO_0.1, MELO_0.2\n",
      "Warning: Agent 0 doesn't have a 'lam' attribute\n",
      "Warning: Agent 1 doesn't have a 'lam' attribute\n",
      "Warning: Agent 2 doesn't have a 'lam' attribute\n",
      "At the end of the simulation we get {-12: tensor(0.), -11: tensor(0.), -10: tensor(0.), -9: tensor(0.), -8: tensor(0.), -7: tensor(0.), -6: tensor(0.), -5: tensor(0.), -4: tensor(0.), -3: tensor(0.), -2: tensor(0.), -1: tensor(0.), 0: tensor(0.), 1: tensor(0.), 2: tensor(0.), 3: tensor(0.)}\n",
      "MELO_ At the end of the simulation we get {-12: 0, -11: 0, -10: 0, -9: 0, -8: 0, -7: 0, -6: 0, -5: 0, -4: 0, -3: 0, -2: 0, -1: 0, 0: 0, 1: 0, 2: 0, 3: 0}\n",
      "Warning: Agent 0 doesn't have a 'lam' attribute\n",
      "Warning: Agent 1 doesn't have a 'lam' attribute\n",
      "Warning: Agent 2 doesn't have a 'lam' attribute\n",
      "At the end of the simulation we get {-12: tensor(0.), -11: tensor(0.), -10: tensor(0.), -9: tensor(0.), -8: tensor(0.), -7: tensor(0.), -6: tensor(0.), -5: tensor(0.), -4: tensor(0.), -3: tensor(0.), -2: tensor(0.), -1: tensor(0.), 0: tensor(0.), 1: tensor(0.), 2: tensor(0.), 3: tensor(0.)}\n",
      "MELO_ At the end of the simulation we get {-12: 0, -11: 0, -10: 0, -9: 0, -8: 0, -7: 0, -6: 0, -5: 0, -4: 0, -3: 0, -2: 0, -1: 0, 0: 0, 1: 0, 2: 0, 3: 0}\n",
      "Warning: Agent 0 doesn't have a 'lam' attribute\n",
      "Warning: Agent 1 doesn't have a 'lam' attribute\n",
      "Warning: Agent 2 doesn't have a 'lam' attribute\n",
      "At the end of the simulation we get {-12: tensor(0.), -11: tensor(0.), -10: tensor(0.), -9: tensor(0.), -8: tensor(0.), -7: tensor(0.), -6: tensor(0.), -5: tensor(0.), -4: tensor(0.), -3: tensor(0.), -2: tensor(0.), -1: tensor(0.), 0: tensor(0.), 1: tensor(0.), 2: tensor(0.), 3: tensor(0.)}\n",
      "MELO_ At the end of the simulation we get {-12: 0, -11: 0, -10: 0, -9: 0, -8: 0, -7: 0, -6: 0, -5: 0, -4: 0, -3: 0, -2: 0, -1: 0, 0: 0, 1: 0, 2: 0, 3: 0}\n",
      "Simulating profile 3/4: MELO_0.1, MELO_0.2, MELO_0.2\n",
      "Warning: Agent 0 doesn't have a 'lam' attribute\n",
      "Warning: Agent 1 doesn't have a 'lam' attribute\n",
      "Warning: Agent 2 doesn't have a 'lam' attribute\n",
      "At the end of the simulation we get {-12: tensor(0.), -11: tensor(0.), -10: tensor(0.), -9: tensor(0.), -8: tensor(0.), -7: tensor(0.), -6: tensor(0.), -5: tensor(0.), -4: tensor(0.), -3: tensor(0.), -2: tensor(0.), -1: tensor(0.), 0: tensor(0.), 1: tensor(0.), 2: tensor(0.), 3: tensor(0.)}\n",
      "MELO_ At the end of the simulation we get {-12: 0, -11: 0, -10: 0, -9: 0, -8: 0, -7: 0, -6: 0, -5: 0, -4: 0, -3: 0, -2: 0, -1: 0, 0: 0, 1: 0, 2: 0, 3: 0}\n",
      "Warning: Agent 0 doesn't have a 'lam' attribute\n",
      "Warning: Agent 1 doesn't have a 'lam' attribute\n",
      "Warning: Agent 2 doesn't have a 'lam' attribute\n",
      "At the end of the simulation we get {-12: tensor(0.), -11: tensor(0.), -10: tensor(0.), -9: tensor(0.), -8: tensor(0.), -7: tensor(0.), -6: tensor(0.), -5: tensor(0.), -4: tensor(0.), -3: tensor(0.), -2: tensor(0.), -1: tensor(0.), 0: tensor(0.), 1: tensor(0.), 2: tensor(0.), 3: tensor(0.)}\n",
      "MELO_ At the end of the simulation we get {-12: 0, -11: 0, -10: 0, -9: 0, -8: 0, -7: 0, -6: 0, -5: 0, -4: 0, -3: 0, -2: 0, -1: 0, 0: 0, 1: 0, 2: 0, 3: 0}\n",
      "Warning: Agent 0 doesn't have a 'lam' attribute\n",
      "Warning: Agent 1 doesn't have a 'lam' attribute\n",
      "Warning: Agent 2 doesn't have a 'lam' attribute\n",
      "At the end of the simulation we get {-12: tensor(0.), -11: tensor(0.), -10: tensor(0.), -9: tensor(0.), -8: tensor(0.), -7: tensor(0.), -6: tensor(0.), -5: tensor(0.), -4: tensor(0.), -3: tensor(0.), -2: tensor(0.), -1: tensor(0.), 0: tensor(0.), 1: tensor(0.), 2: tensor(0.), 3: tensor(0.)}\n",
      "MELO_ At the end of the simulation we get {-12: 0, -11: 0, -10: 0, -9: 0, -8: 0, -7: 0, -6: 0, -5: 0, -4: 0, -3: 0, -2: 0, -1: 0, 0: 0, 1: 0, 2: 0, 3: 0}\n",
      "Simulating profile 4/4: MELO_0.2, MELO_0.2, MELO_0.2\n",
      "Warning: Agent 0 doesn't have a 'lam' attribute\n",
      "Warning: Agent 1 doesn't have a 'lam' attribute\n",
      "Warning: Agent 2 doesn't have a 'lam' attribute\n",
      "At the end of the simulation we get {-12: tensor(0.), -11: tensor(0.), -10: tensor(0.), -9: tensor(0.), -8: tensor(0.), -7: tensor(0.), -6: tensor(0.), -5: tensor(0.), -4: tensor(0.), -3: tensor(0.), -2: tensor(0.), -1: tensor(0.), 0: tensor(0.), 1: tensor(0.), 2: tensor(0.), 3: tensor(0.)}\n",
      "MELO_ At the end of the simulation we get {-12: 0, -11: 0, -10: 0, -9: 0, -8: 0, -7: 0, -6: 0, -5: 0, -4: 0, -3: 0, -2: 0, -1: 0, 0: 0, 1: 0, 2: 0, 3: 0}\n",
      "Warning: Agent 0 doesn't have a 'lam' attribute\n",
      "Warning: Agent 1 doesn't have a 'lam' attribute\n",
      "Warning: Agent 2 doesn't have a 'lam' attribute\n",
      "At the end of the simulation we get {-12: tensor(0.), -11: tensor(0.), -10: tensor(0.), -9: tensor(0.), -8: tensor(0.), -7: tensor(0.), -6: tensor(0.), -5: tensor(0.), -4: tensor(0.), -3: tensor(0.), -2: tensor(0.), -1: tensor(0.), 0: tensor(0.), 1: tensor(0.), 2: tensor(0.), 3: tensor(0.)}\n",
      "MELO_ At the end of the simulation we get {-12: 0, -11: 0, -10: 0, -9: 0, -8: 0, -7: 0, -6: 0, -5: 0, -4: 0, -3: 0, -2: 0, -1: 0, 0: 0, 1: 0, 2: 0, 3: 0}\n",
      "Warning: Agent 0 doesn't have a 'lam' attribute\n",
      "Warning: Agent 1 doesn't have a 'lam' attribute\n",
      "Warning: Agent 2 doesn't have a 'lam' attribute\n",
      "At the end of the simulation we get {-12: tensor(0.), -11: tensor(0.), -10: tensor(0.), -9: tensor(0.), -8: tensor(0.), -7: tensor(0.), -6: tensor(0.), -5: tensor(0.), -4: tensor(0.), -3: tensor(0.), -2: tensor(0.), -1: tensor(0.), 0: tensor(0.), 1: tensor(0.), 2: tensor(0.), 3: tensor(0.)}\n",
      "MELO_ At the end of the simulation we get {-12: 0, -11: 0, -10: 0, -9: 0, -8: 0, -7: 0, -6: 0, -5: 0, -4: 0, -3: 0, -2: 0, -1: 0, 0: 0, 1: 0, 2: 0, 3: 0}\n",
      "[(0, 'MELO_0.1', 0), (1, 'MELO_0.1', 0), (2, 'MELO_0.1', 0)]\n",
      "[(0, 'MELO_0.1', 0), (1, 'MELO_0.1', 0), (2, 'MELO_0.1', 0)]\n",
      "[(0, 'MELO_0.1', 0), (1, 'MELO_0.1', 0), (2, 'MELO_0.1', 0)]\n",
      "[(0, 'MELO_0.1', 0), (1, 'MELO_0.1', 0), (2, 'MELO_0.2', 0)]\n",
      "[(0, 'MELO_0.1', 0), (1, 'MELO_0.1', 0), (2, 'MELO_0.2', 0)]\n",
      "[(0, 'MELO_0.1', 0), (1, 'MELO_0.1', 0), (2, 'MELO_0.2', 0)]\n",
      "[(0, 'MELO_0.1', 0), (1, 'MELO_0.2', 0), (2, 'MELO_0.2', 0)]\n",
      "[(0, 'MELO_0.1', 0), (1, 'MELO_0.2', 0), (2, 'MELO_0.2', 0)]\n",
      "[(0, 'MELO_0.1', 0), (1, 'MELO_0.2', 0), (2, 'MELO_0.2', 0)]\n",
      "[(0, 'MELO_0.2', 0), (1, 'MELO_0.2', 0), (2, 'MELO_0.2', 0)]\n",
      "[(0, 'MELO_0.2', 0), (1, 'MELO_0.2', 0), (2, 'MELO_0.2', 0)]\n",
      "[(0, 'MELO_0.2', 0), (1, 'MELO_0.2', 0), (2, 'MELO_0.2', 0)]\n",
      "Found 4 repeat profiles in data\n",
      "Profile (3, 0): Strategy MELO_0.1 has 9 payoffs with mean 0.0000\n",
      "Profile (2, 1): Strategy MELO_0.1 has 6 payoffs with mean 0.0000\n",
      "Profile (2, 1): Strategy MELO_0.2 has 3 payoffs with mean 0.0000\n",
      "Profile (1, 2): Strategy MELO_0.1 has 3 payoffs with mean 0.0000\n",
      "Profile (1, 2): Strategy MELO_0.2 has 6 payoffs with mean 0.0000\n",
      "Profile (0, 3): Strategy MELO_0.2 has 9 payoffs with mean 0.0000\n",
      "Raw payoff table:\n",
      "Config 1: [MELO_0.1: 3] → [MELO_0.1: 0.00]\n",
      "Config 2: [MELO_0.1: 2, MELO_0.2: 1] → [MELO_0.1: 0.00, MELO_0.2: 0.00]\n",
      "Config 3: [MELO_0.1: 1, MELO_0.2: 2] → [MELO_0.1: 0.00, MELO_0.2: 0.00]\n",
      "Config 4: [MELO_0.2: 3] → [MELO_0.2: 0.00]\n",
      "Game created with 3 players and 2 strategies\n",
      "Strategy names: ['MELO_0.1', 'MELO_0.2']\n",
      "| Profile   |   MELO_0.1 Count |   MELO_0.2 Count | MELO_0.1 Payoff   | MELO_0.2 Payoff   |\n",
      "|:----------|-----------------:|-----------------:|:------------------|:------------------|\n",
      "| Profile 1 |                3 |                0 | 0.0000            | N/A               |\n",
      "| Profile 2 |                2 |                1 | 0.0000            | 0.0000            |\n",
      "| Profile 3 |                1 |                2 | 0.0000            | 0.0000            |\n",
      "| Profile 4 |                0 |                3 | N/A               | 0.0000            |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabesmithline/Desktop/SRG/melo_project/marketsim/egta/symmetric_game.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.config_table = torch.tensor(config_table, dtype=torch.float32, device=device)\n",
      "/Users/gabesmithline/Desktop/SRG/melo_project/marketsim/egta/symmetric_game.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.payoff_table = torch.tensor(payoff_table, dtype=torch.float32, device=device)\n"
     ]
    }
   ],
   "source": [
    "# Define initial strategies to explore\n",
    "initial_strategies = [\n",
    "        {\"name\": \"MELO_0.1\", \"param\": 0.1},  # Strategy with lambda = 0.1\n",
    "        {\"name\": \"MELO_0.2\", \"param\": 0.2}   # Strategy with lambda = 0.2\n",
    "    ]\n",
    "    \n",
    "    # Define full strategy space (for later expansion in Quiesce)\n",
    "strategy_space = [\n",
    "        {\"name\": f\"MELO_{lamb:.1f}\", \"param\": lamb} \n",
    "        for lamb in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    ]\n",
    "    \n",
    "    # Parameters for simulation\n",
    "num_players = 3  # Small number for initial test\n",
    "num_runs = 3     # Multiple runs per profile for statistical significance\n",
    "    \n",
    "    # Run initial simulations\n",
    "simulation_results = run_initial_simulations(\n",
    "        strategies=initial_strategies,\n",
    "        num_players=num_players,\n",
    "        num_runs=num_runs\n",
    "    )\n",
    "    \n",
    "    # Process results into a SymmetricGame\n",
    "symmetric_game = create_symmetric_game_from_data(simulation_results)\n",
    "\n",
    "# Print game information\n",
    "print(f\"Game created with {symmetric_game.num_players} players and {symmetric_game.num_actions} strategies\")\n",
    "print(f\"Strategy names: {symmetric_game.strategy_names}\")\n",
    "\n",
    "# To print the payoff table\n",
    "if hasattr(symmetric_game, 'print_full_heuristic_payoff_table'):\n",
    "    symmetric_game.print_full_heuristic_payoff_table()\n",
    "else:\n",
    "    print(\"Payoff matrix shape:\", symmetric_game.payoff_table.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Profile   |   MELO_0.1 Count |   MELO_0.2 Count | MELO_0.1 Payoff   | MELO_0.2 Payoff   |\n",
      "|:----------|-----------------:|-----------------:|:------------------|:------------------|\n",
      "| Profile 1 |                3 |                0 | 0.0000            | N/A               |\n",
      "| Profile 2 |                2 |                1 | 0.0000            | 0.0000            |\n",
      "| Profile 3 |                1 |                2 | 0.0000            | 0.0000            |\n",
      "| Profile 4 |                0 |                3 | N/A               | 0.0000            |\n"
     ]
    }
   ],
   "source": [
    "symmetric_game.print_full_heuristic_payoff_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scheduler to handle on-demand simulations during Quiesce\n",
    "scheduler = (\n",
    "    strategy_names=strategy_names,\n",
    "    num_players=symmetric_game.num_players,\n",
    "    simulator_path=\"python -m marketsim.simulator.melo_simulator\",\n",
    "    simulator_config={\n",
    "        \"num_background_agents\": 100,\n",
    "        \"sim_time\": 1000\n",
    "        # Other simulator parameters\n",
    "    }\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
