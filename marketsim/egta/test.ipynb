{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)\n",
    "from egta.symmetric_game import *\n",
    "from egta.game import *\n",
    "from egta.utils.eq_computation import *\n",
    "from egta.utils.log_multimodal import *\n",
    "from egta.utils.random_functions import *\n",
    "from egta.utils.simplex_operations import *\n",
    "from egta.process_data import *\n",
    "import matplotlib.pyplot as plt\n",
    "from egta.reductions.dpr import DPRGAME\n",
    "import random\n",
    "import itertools\n",
    "from game_simulators_test import games\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display the data\n",
    "rps_profiles = games.generate_rps_25_player_data()#generate_blotto_data(num_players=25, num_battlefields=2, num_troops=2)#generate_rps_25_player_data()\n",
    "print(rps_profiles)\n",
    "rps_data = rps_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create symmetric game from the data\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "rps_game = create_symmetric_game_from_data(rps_data, device=device)\n",
    "print(rps_game.num_players)\n",
    "print(rps_game.num_actions)\n",
    "print(rps_game.strategy_names)\n",
    "print(rps_game.config_table)\n",
    "print(rps_game.payoff_table)\n",
    "print(rps_game.print_full_heuristic_payoff_table())\n",
    "print(f\"Created RPS game with {rps_game.num_players} players and {rps_game.num_actions} strategies\")\n",
    "print(f\"Strategy names: {rps_game.strategy_names}\")\n",
    "\n",
    "#find Nash equilibrium using replicator dynamics\n",
    "iters = 100\n",
    "\n",
    "#generate random mixture\n",
    "random_mixture = torch.rand(rps_game.num_actions, device=rps_game.device)\n",
    "random_mixture = simplex_normalize(random_mixture)\n",
    "print(f\"Starting random mixture: {random_mixture}\")\n",
    "\n",
    "#run fictitious play\n",
    "#NOTE: very sensitive to step size, and the initial mixture\n",
    "#FP works, gradient descent works, rd works\n",
    "#Not better response does not work?\n",
    "\n",
    "'''\n",
    "NOTE:, very sensitive to step size, and the initial mixture, and other hyperparameters\n",
    "'''\n",
    "eq_mixture, eq_history = logged_replicator_dynamics(rps_game, random_mixture, iters=iters, offset=.5)\n",
    "print(eq_history)\n",
    "print(\"\\nNash Equilibrium:\")\n",
    "for i, strat in enumerate(rps_game.strategy_names):\n",
    "    print(f\"{strat}: {eq_mixture[i].item():.4f}\")\n",
    "\n",
    "#print(f\"\\nMaximum deviation from uniform: {torch.max(torch.abs(eq_mixture - 1/rps_game.num_actions)).item():.6f}\")\n",
    "\n",
    "# Calculate regret at the equilibrium\n",
    "regret = rps_game.regret(eq_mixture)\n",
    "print(f\"Regret at equilibrium: {regret.item():.6f}\")\n",
    "\n",
    "# Plot convergence history\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, strat in enumerate(rps_game.strategy_names):\n",
    "    plt.plot(eq_history[i, :].cpu().numpy(), label=strat)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Strategy Probability\")\n",
    "plt.legend()\n",
    "plt.title(\"Replicator Dynamics Convergence\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Plot regret over iterations\n",
    "regrets = torch.zeros(iters+1, device=device)\n",
    "for i in range(iters+1):\n",
    "    regrets[i] = rps_game.regret(eq_history[:, i])\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(regrets.cpu().numpy())\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Regret\")\n",
    "plt.title(\"Regret Convergence\")\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct DPR Game From Symmetric Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpr_rps_game = DPRGAME(rps_game, 5) \n",
    "\n",
    "#solve dpr_game \n",
    "#game, mix, iters=1000, offset=0\n",
    "eq_mixture, eq_history = logged_replicator_dynamics(dpr_rps_game, random_mixture, offset=.5)\n",
    "print(dpr_rps_game.payoff_table)\n",
    "print(\"\\nNash Equilibrium:\")\n",
    "for i, strat in enumerate(dpr_rps_game.strategy_names):\n",
    "    print(f\"{strat}: {eq_mixture[i].item():.4f}\")\n",
    "\n",
    "print(f\"\\nMaximum deviation from uniform: {torch.max(torch.abs(eq_mixture - 1/dpr_rps_game.num_actions)).item():.6f}\")\n",
    "\n",
    "# Calculate regret at the equilibrium\n",
    "regret = rps_game.regret(eq_mixture)\n",
    "print(f\"Regret at equilibrium: {regret.item():.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
